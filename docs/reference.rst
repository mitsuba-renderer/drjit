.. py:module:: drjit

.. _reference:

API Reference
=============

Array creation
--------------

.. autofunction:: zeros
.. autofunction:: empty
.. autofunction:: ones
.. autofunction:: full
.. autofunction:: opaque
.. autofunction:: arange
.. autofunction:: linspace

Control flow
------------

.. autofunction:: syntax
.. autofunction:: hint
.. autofunction:: while_loop
.. autofunction:: if_stmt
.. autofunction:: switch
.. autofunction:: dispatch

.. _horizontal-reductions-ref:

Horizontal operations
---------------------

These operations are *horizontal* in the sense that [..]

.. autofunction:: gather
.. autofunction:: scatter

.. autoclass:: ReduceOp

   .. autoattribute:: Identity
      :noindex:
      :annotation:

      Perform an ordinary scatter operation that ignores the current entry.

   .. autoattribute:: Add
      :annotation:

      Addition.

   .. autoattribute:: Mul
      :annotation:

      Multiplication.

   .. autoattribute:: Min
      :annotation:

      Minimum

   .. autoattribute:: Max
      :annotation:

      Maximum

   .. autoattribute:: And
      :annotation:

      Binary AND operation

   .. autoattribute:: Or
      :annotation:

      Binary OR operation

.. autoclass:: ReduceMode

   .. autoattribute:: Auto
      :annotation:

      Select the first valid option from the following list:

      - use :py:attr:`drjit.ReduceMode.Expand` if the computation uses the LLVM
        backend and the ``target`` array storage size (in MiB) is smaller or equal
        than than the value given by :py:func:`drjit.expand_threshold`. This
        threshold can be changed using the :py:func:`drjit.set_expand_threshold`
        function.

      - use :py:attr:`drjit.ReduceMode.Local` if
        :py:attr:`drjit.JitFlag.ScatterReduceLocal` is set.

      - fall back to :py:attr:`drjit.ReduceMode.Direct`.

   .. autoattribute:: Direct
      :annotation:

      Insert an ordinary atomic reduction operation into the program.

      This mode is ideal when no or little contention is expected, for example
      because the target indices of scatters are well spread throughout the
      target array. This mode generates a minimal amount of code, which can
      help improve performance especially on GPU backends.

   .. autoattribute:: Local
      :annotation:

      Locally pre-reduce operands.

      In this mode, Dr.Jit adds extra code to the compiled program to examine the
      target indices of atomic updates. For example, CUDA programs run with an
      instruction granularity referred to as a *warp*, which is a group of 32
      threads. When some of these threads want to write to the same location, then
      those operands can be pre-processed to reduce the total number of necessary
      atomic memory transactions (potentially to just a single one!)

      On the CPU/LLVM backend, the same process works at the granularity of
      *packets*. The details depends on the underlying instruction set---for
      example, there are 16 threads per packet on a machine with AVX512, so
      there is a potential for reducing atomic write traffic by that factor.)";

   .. autoattribute:: NoConflicts
      :annotation:

      Perform a non-atomic read-modify-write operation.

      This mode is only safe in specific situations. The caller must guarantee
      that there are no conflicts (i.e., scatters targeting the same elements).
      If specified, Dr.Jit will generate a *non-atomic* read-modify-update
      operation that potentially runs significantly faster, especially on the
      LLVM backend.

   .. autoattribute:: Expand
      :annotation:

      Expand the target array to avoid write conflicts, then scatter
      non-atomically.

      This feature is only supported on the LLVM backend. Other backends
      interpret this flag as if :py:attr:`drjit.ReduceMode.Auto` had been
      specified.

      This mode internally expands the storage underlying the target array to a
      much larger size that is proportional to the number of CPU cores. Scalar
      (length-1) target arrays are expanded even further to ensure that each
      CPU gets an entirely separate cache line.

      Following this one-time expansion step, the array can then accommodate an
      arbitrary sequence of scatter-reduction operations that the system will
      internally perform using *non-atomic* read-modify-write operations (i.e.,
      analogous to the :py:attr:`NoConflicts` mode). Subsequent read accesses
      re-compress the array into the ordinary representation.

      On bigger arrays and on machines with many cores, the storage costs
      resulting from this mode can be prohibitive.

   .. autoattribute:: Permute
      :annotation:

      In contrast to prior enumeration entries, this one modifies plain
      (non-reductive) scatters and gathers. It exists to enable internal
      optimizations that Dr.Jit uses when differentiating vectorized function
      calls and compressed loops. You likely should not use it in your own code.

      When setting this mode, the caller guarantees that there will be no
      conflicts, and that every entry is written exactly single time using an
      index vector representing a permutation (it's fine this permutation is
      accomplished by multiple separate write operations, but there should be no
      more than 1 write to each element).

      Giving 'Permute' as an argument to a (nominally read-only) gather
      operation is helpful because we then know that the reverse-mode derivative
      of this operation can be a plain scatter instead of a more costly
      atomic scatter-add.

      Giving 'Permute' as an argument to a scatter operation is helpful
      because we then know that the forward-mode derivative does not depend
      on any prior derivative values associated with that array, as all
      current entries will be overwritten.

.. autofunction:: scatter_reduce
.. autofunction:: scatter_add
.. autofunction:: scatter_add_kahan
.. autofunction:: scatter_inc
.. autofunction:: compress
.. autofunction:: ravel
.. autofunction:: unravel
.. autofunction:: reshape
.. autofunction:: slice
.. autofunction:: tile
.. autofunction:: repeat
.. autofunction:: min
.. autofunction:: max
.. autofunction:: sum
.. autofunction:: mean
.. autofunction:: prod
.. autofunction:: dot
.. autofunction:: abs_dot
.. autofunction:: squared_norm
.. autofunction:: norm
.. autofunction:: all
.. autofunction:: any
.. autofunction:: none
.. autofunction:: count
.. autofunction:: prefix_sum
.. autofunction:: block_sum
.. autofunction:: cumsum
.. autofunction:: reverse

Mask operations
---------------

Also relevant here are :py:func:`any`, :py:func:`all`, :py:func:`none`, and :py:func:`count`.

.. autofunction:: select
.. autofunction:: isinf
.. autofunction:: isnan
.. autofunction:: isfinite
.. autofunction:: allclose

Miscellaneous operations
------------------------

.. autofunction:: shape
.. autofunction:: width
.. autofunction:: slice_index
.. autofunction:: meshgrid
.. autofunction:: make_opaque
.. autofunction:: copy

Just-in-time compilation
------------------------

.. autoclass:: JitBackend

   .. autoattribute:: None
      :noindex:

      Indicates that a type is not handled by a Dr.Jit backend (e.g., a scalar type)

   .. autoattribute:: LLVM
      :annotation:

      Dr.Jit backend targeting various processors via the LLVM compiler infrastructure.

   .. autoattribute:: CUDA
      :annotation:

      Dr.Jit backend targeting NVIDIA GPUs using PTX ("Parallel Thread Execution") IR.

.. autoclass:: VarType

   .. autoattribute:: Void
      :annotation:

      Unknown/unspecified type.

   .. autoattribute:: Bool
      :annotation:

      Boolean/mask type.

   .. autoattribute:: Int8
      :annotation:

      Signed 8-bit integer.

   .. autoattribute:: UInt8
      :annotation:

      Unsigned 8-bit integer.

   .. autoattribute:: Int16
      :annotation:

      Signed 16-bit integer.

   .. autoattribute:: UInt16
      :annotation:

      Unsigned 16-bit integer.

   .. autoattribute:: Int32
      :annotation:

      Signed 32-bit integer.

   .. autoattribute:: UInt32
      :annotation:

      Unsigned 32-bit integer.

   .. autoattribute:: Int64
      :annotation:

      Signed 64-bit integer.

   .. autoattribute:: UInt64
      :annotation:

      Unsigned 64-bit integer.

   .. autoattribute:: Pointer
      :annotation:

      Pointer to a memory address.

   .. autoattribute:: Float16
      :annotation:

      16-bit floating point format (IEEE 754).

   .. autoattribute:: Float32
      :annotation:

      32-bit floating point format (IEEE 754).

   .. autoattribute:: Float64
      :annotation:

      64-bit floating point format (IEEE 754).

.. autoclass:: VarState

   .. autoattribute:: Invalid
      :annotation:

      The variable has length 0 and effectively does not exist.

   .. autoattribute:: Undefined
      :annotation:

      An undefined memory region. Does not (yet) consume device memory.

   .. autoattribute:: Literal
      :annotation:

      A literal constant. Does not consume device memory.

   .. autoattribute:: Unevaluated
      :annotation:

      An ordinary unevaluated variable that is neither a literal constant nor symbolic.

   .. autoattribute:: Evaluated
      :annotation:

      An evaluated variable backed by a device memory region.

   .. autoattribute:: Dirty
      :annotation:

      An evaluated variable backed by a device memory region. The variable
      furthermore has pending *side effects* (i.e. the user has performed a
      :py:func:`drjit.scatter`, :py:func:`drjit.scatter_reduce`
      :py:func:`drjit.scatter_inc`, :py:func:`drjit.scatter_add`, or
      :py:func:`drjit.scatter_add_kahan` operation, and the effect of this
      operation has not been realized yet). The array's status will
      automatically change to :py:attr:`Evaluated` the next time that Dr.Jit
      evaluates computation, e.g. via :py:func:`drjit.eval`.

   .. autoattribute:: Symbolic
      :annotation:

      A symbolic variable that could take on various inputs. Cannot be evaluated.

   .. autoattribute:: Mixed
      :annotation:

      This is a nested array, and the components have mixed states.

.. autofunction:: has_backend
.. autofunction:: schedule
.. autofunction:: eval

.. autoclass:: JitFlag

   .. autoattribute:: Debug
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      **Debug mode**: Enable functionality to uncover errors in application code.

      When debug mode is enabled, Dr.Jit inserts a number of additional runtime
      checks to locate sources of undefined behavior.

      Debug mode comes at a *significant* cost: it interferes with kernel caching,
      reduces tracing performance, and produce kernels that run slower. We recommend
      that you only use it periodically before a release, or when encountering a
      serious problem like a crashing kernel.

      First, debug mode enables assertion checks in user code such as those performed by
      :py:func:`drjit.assert_true`, :py:func:`drjit.assert_false`, and
      :py:func:`drjit.assert_equal`.

      Second, Dr.Jit inserts additional checks to intercept out-of-bound reads
      and writes performed by operations such as :py:func:`drjit.scatter`,
      :py:func:`drjit.gather`, :py:func:`drjit.scatter_reduce`,
      :py:func:`drjit.scatter_inc`, etc. It also detects calls to invalid callables
      performed via :py:func:`drjit.switch`, :py:func:`drjit.dispatch`. Such invalid
      operations are masked, and they generate a warning message on the console,
      e.g.:

      .. code-block:: pycon
         :emphasize-lines: 2-3

         >>> dr.gather(dtype=UInt, source=UInt(1, 2, 3), index=UInt(0, 1, 100))
         RuntimeWarning: drjit.gather(): out-of-bounds read from position 100 in an array⏎
         of size 3. (<stdin>:2)

      Finally, Dr.Jit also installs a `python tracing hook
      <https://docs.python.org/3/library/sys.html#sys.settrace>`__ that
      associates all Jit variables with their Python source code location, and
      this information is propagated all the way to the final intermediate
      representation (PTX, LLVM IR). This is useful for low-level debugging and
      development of Dr.Jit itself. You can query the source location
      information of a variable ``x`` by writing ``x.label``.

      Due to limitations of the Python tracing interface, this handler becomes active
      within the *next* called function (or Jupyter notebook cell) following
      activation of the :py:attr:`drjit.JitFlag.Debug` flag. It does not apply to
      code within the same scope/function.

      C++ code using Dr.Jit also benefits from debug mode but will lack accurate
      source code location information. In mixed-language projects, the reported file
      and line number information will reflect that of the last operation on the
      Python side of the interface.

   .. autoattribute:: ReuseIndices
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      **Index reuse**: Dr.Jit consists of two main parts: the just-in-time
      compiler, and the automatic differentiation layer. Both maintain an
      internal data structure representing captured computation, in which each
      variable is associated with an index (e.g., ``r1234`` in the JIT
      compiler, and ``a1234`` in the AD graph).

      The index of a Dr.Jit array in these graphs can be queried via the
      :py:attr:`ArrayBase.index` and :py:attr:`ArrayBase.index_ad` variables,
      and they are also visible in debug messages (if
      :py:func:`drjit.set_log_level` is set to a more verbose debug level).

      Dr.Jit aggressively reuses the indices of expired variables by default,
      but this can make debug output difficult to interpret. When when
      debugging Dr.Jit itself, it is often helpful to investigate the history
      of a particular variable. In such cases, set this flag to ``False`` to
      disable variable reuse both at the JIT and AD levels. This comes at a
      cost: the internal data structures keep on growing, so it is not suitable
      for long-running computations.

      Index reuse is *enabled* by default.

   .. autoattribute:: ConstantPropagation
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      **Constant propagation**: immediately evaluate arithmetic involving
      literal constants on the host and don't generate any device code for
      them.

      For example, the following assertion holds when value numbering is
      enabled in Dr.Jit.

      .. code-block:: python

         from drjit.llvm import Int

         # Create two literal constant arrays
         a, b = Int(4), Int(5)

         # This addition operation can be immediately performed and does
         # not need to be recorded
         c1 = a + b

         # Double-check that c1 and c2 refer to the same Dr.Jit variable
         c2 = Int(9)
         assert c1.index == c2.index

      Constant propagation is *enabled* by default.

   .. autoattribute:: ValueNumbering
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      **Local value numbering**: a simple variant of common subexpression
      elimination that collapses identical expressions within basic blocks. For
      example, the following assertion holds when value numbering is enabled in
      Dr.Jit.

      .. code-block:: python

         from drjit.llvm import Int

         # Create two nonliteral arrays stored in device memory
         a, b = Int(1, 2, 3), Int(4, 5, 6)

         # Perform the same arithmetic operation twice
         c1 = a + b
         c2 = a + b

         # Verify that c1 and c2 reference the same Dr.Jit variable
         assert c1.index == c2.index

      Local value numbering is *enabled* by default.

   .. autoattribute:: FastMath
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      **Fast Math**: this flag is analogous to the ``-ffast-math`` flag in C
      compilers. When set, the system may use approximations and simplifications
      that sacrifice strict IEEE-754 compatibility.

      Currently, it changes two behaviors:

      - expressions of the form ``a * 0`` will be simplified to ``0`` (which is
        technically not correct when ``a`` is infinite or NaN-valued).

      - Dr.Jit will use slightly approximate division and square root
        operations for single precision arguments in CUDA mode. Disabling fast
        math mode is costly on CUDA devices, as the strict IEEE-754 compliant
        version of these operations requires a software-emulated fallback.

      Fast math mode is *enabled* by default.

   .. autoattribute:: SymbolicCalls
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      Dr.Jit provides two main ways of compiling function calls targeting
      *instance arrays*.

      1. **Symbolic mode** (the default): Dr.Jit invokes each callable with
         *symbolic* (abstract) arguments. It does this to capture a transcript
         of the computation that it can turn into a function in the generated
         kernel. Symbolic mode preserves the control flow structure of the
         original program by replicating it within Dr.Jit's intermediate
         representation.

      2. **Evaluated mode**: Dr.Jit evaluates all inputs and groups them by instance
         ID. Following this, it launches a a kernel *per instance* to process the
         rearranged inputs and assemble the function return value.

      A separate section about :ref:`symbolic and evaluated modes <sym-eval>`
      discusses these two options in detail.

      Besides calls to instance arrays, this flag also controls the behavior of
      the functions :py:func:`drjit.switch` and :py:func:`drjit.dispatch`.

      Symbolic calls are *enabled* by default.

   .. autoattribute:: OptimizeCalls
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      Perform basic optimizations for function calls on instance arrays.

      This flag enables two optimizations:

      - *Constant propagation*: Dr.Jit will propagate literal constants across
        function boundaries while tracing, which can unlock simplifications
        within. This is especially useful in combination with automatic
        differentiation, where it helps to detect code that does not influence
        the computed derivatives.

      - *Devirtualization*: When an element of the return value has the same
        computation graph in all instances, it is removed from the function
        call interface and moved to the caller.

      The flag is *enabled* by default. Note that it is only meaningful in
      combination with :py:attr:`SymbolicCalls`. Besides calls to instance
      arrays, this flag also controls the behavior of the functions
      :py:func:`drjit.switch` and :py:func:`drjit.dispatch`.

   .. autoattribute:: MergeFunctions
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      Deduplicate code generated by function calls on instance arrays.

      When ``arr`` is an instance array (potentially with thousands of
      instances), a function call like

      .. code-block:: python

         arr.f(inputs...)

      can potentially generate vast numbers of different callables in the
      generated code. At the same time, many of these callables may contain
      identical code (or code that is identical except for data references).

      Dr.Jit can exploit such redundancy and merge such callables during code
      generation. Besides generating shorter programs, this also helps to
      reduce thread divergence.

      This flag is *enabled* by default. Note that it is only meaningful in
      combination with :py:attr:`SymbolicCalls`. Besides calls to instance
      arrays, this flag also controls the behavior of the functions
      :py:func:`drjit.switch` and :py:func:`drjit.dispatch`.

   .. autoattribute:: SymbolicLoops
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      Dr.Jit provides two main ways of compiling loops involving Dr.Jit arrays.

      1. **Symbolic mode** (the default): Dr.Jit executes the loop a single
         time regardless of how many iterations it requires in practice. It
         does so with *symbolic* (abstract) arguments to capture the loop
         condition and body and then turns it into an equivalent loop in the
         generated kernel. Symbolic mode preserves the control flow structure
         of the original program by replicating it within Dr.Jit's intermediate
         representation.

      2. **Evaluated mode**: Dr.Jit evaluates the loop's state variables and
         reduces the loop condition to a single element (``bool``) that
         expresses whether any elements are still alive. If so, it runs the
         loop body and the process repeats.

      A separate section about :ref:`symbolic and evaluated modes <sym-eval>`
      discusses these two options in detail.

      Symbolic loops are *enabled* by default.

   .. autoattribute:: OptimizeLoops
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      Perform basic optimizations for loops involving Dr.Jit arrays.

      This flag enables two optimizations:

      - *Constant arrays*: variables in the *loop state* set that aren't
        modified by a loop are removed from this set. This shortens the
        generated code, which can be helpful especially in combination with the
        automatic transformations performed by :py:func:`drjit.function` that
        may be somewhat conservative in classifying too many local variables as
        potential loop state.

      - *Literal constant arrays*: In addition to the above point, constant
        loop state variables that are *literal constants* are propagated into
        the loop body, where this may reveal optimization opportunities.

        This is useful in combination with automatic differentiation, where it
        helps to detect code that does not influence the computed derivatives.

      One practical implication of this optimization is that it may cause
      :py:func:`drjit.while_loop` to run the loop body twice instead of just
      once.

      This flag is enabled by default. Note that it is only meaningful in
      combination with  :py:attr:`SymbolicLoops`.

   .. autoattribute:: CompressLoops
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      Compress the loop state of evaluated loops after every iteration.

      When an evaluated loop processes many elements, and when each element
      requires a different number of loop iterations, there is question of what
      should be done with inactive elements. The default implementation keeps
      them around and does redundant calculations that are, however, masked
      out. Consequently, later loop iterations don't run faster despite fewer
      elements being active.

      Setting this falg causes the removal of inactive elements after every
      iteration. This reorganization is not for free and does not benefit all
      use cases.

      This flag is *disabled* by default. Note that it only applies to
      *evaluated* loops (i.e., when :py:attr:`SymbolicLoops` is disabled, or
      the ``mode='evaluted'`` parameter as passed to the loop in question).

   .. autoattribute:: SymbolicConditionals
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      Dr.Jit provides two main ways of compiling conditionals involving Dr.Jit arrays.

      1. **Symbolic mode** (the default): Dr.Jit captures the computation
         performed by the ``True`` and ``False`` branches and generates an
         equivalent branch in the generated kernel. Symbolic mode preserves the
         control flow structure of the original program by replicating it
         within Dr.Jit's intermediate representation.

      2. **Evaluated mode**: Dr.Jit always executes both branches and blends
         their outputs.

      A separate section about :ref:`symbolic and evaluated modes <sym-eval>`
      discusses these two options in detail.

      Symbolic conditionals are *enabled* by default.

   .. autoattribute:: ForceOptiX
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      Force execution through OptiX even if a kernel doesn't use ray tracing.
      This only applies to the CUDA backend is mainly helpful for automated
      tests done by the Dr.Jit team.

      This flag is *disabled* by default.

   .. autoattribute:: PrintIR
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      Print the low-level IR representation when launching a kernel.

      If enabled, this flag causes Dr.Jit to print the low-level IR (LLVM IR,
      NVIDIA PTX) representation of the generated code onto the console (or
      Jupyter notebook).

      This flag is *disabled* by default.

   .. autoattribute:: KernelHistory
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      Maintain a history of kernel launches to profile/debug programs.

      Programs written on top of Dr.Jit execute in an *extremely* asynchronous
      manner. By default, the system postpones the computation to build large
      fused kernels. Even when this computation eventually runs, it does so
      asynchronously with respect to the host, which can make benchmarking
      difficult.

      In general, beware of the following benchmarking *anti-pattern*:

      .. code-block::

          import time
          a = time.time()
          # Some Dr.Jit computation
          b = time.time()
          print("took %.2f ms" % ((b-a) * 1000))

      In the worst case, the measured time interval may only capture the
      *tracing time*, without any actual computation having taken place.
      Another common mistake with this pattern is that Dr.Jit or the target
      device may still be busy with computation that started *prior* to the ``a
      = time.time()`` line, which is now incorrectly added to the measured
      period.

      Dr.Jit provides a *kernel history* feature, where it creates an entry in
      a list whenever it launches a kernel or related operation (memory copies,
      etc.). This not only gives accurate and isolated timings (measured with
      counters on the CPU/GPU) but also reveals if a kernel was launched at
      all. To capture the kernel history, set this flag just before the region
      to be benchmarked and call :py:func:`drjit.kernel_history()` at the end.

      Capturing the history has a (very) small cost and is therefore
      *disabled* by default.

   .. autoattribute:: LaunchBlocking
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      Force synchronization after every kernel launch. This is useful to
      isolate severe problems (e.g. crashes) to a specific kernel.

      This flag has a severe performance impact and is *disabled* by default.

   .. autoattribute:: ScatterReduceLocal
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      Reduce locally before performing atomic scatter-reductions.

      Atomic memory operations are expensive when many writes target the same
      region of memory. This leads to a phenomenon called *contention* that is
      normally associated with significant slowdowns (10-100x aren't unusual).

      This issue is particularly common when automatically differentiating
      computation in *reverse mode* (e.g. :py:func:`drjit.backward`), since
      this transformation turns differentiable global memory reads into atomic
      scatter-additions. A differentiable scalar read is all it takes to create
      such an atomic memory bottleneck.

      To reduce this cost, Dr.Jit can perform a *local reduction* that
      uses cooperation between SIMD/warp lanes to resolve all requests
      targeting the same address and then only issuing a single atomic memory
      transaction per unique target. This can reduce atomic memory traffic
      32-fold on the GPU (CUDA) and 16-fold on the CPU (AVX512).

      The section on :ref:`optimizations <reduce-local>` presents plots that
      demonstrate the impact of this optimization.

      The JIT flag :py:attr:`drjit.JitFlag.ScatterReduceLocal` affects the
      behavior of :py:func:`scatter_add`, :py:func:`scatter_reduce` along with
      the reverse-mode derivative of :py:func:`gather`.
      Setting the flag to ``True`` will usually cause a ``mode=`` argument
      value of :py:attr:`drjit.ReduceOp.Auto` to be interpreted as
      :py:attr:`drjit.ReduceOp.Local`. Another LLVM-specific optimization
      takes precedence in certain sitatuations,
      refer to the discussion of this flag for details.

      This flag is *enabled* by default.

   .. autoattribute:: SymbolicScope
      :annotation:

      .. For Sphinx-related technical reasons, the below comment is replicated
         in docstr.h. Please keep the two in sync when making changes.

      This flag is set to ``True`` when Dr.Jit is currently capturing symbolic
      computation. The flag is automatically managed and should not be updated
      by application code.

      User code may query this flag to check if it is legal to perform certain
      operations (e.g., evaluating array contents).

      Note that this information can also be queried in a more fine-grained
      manner (per variable) using the :py:attr:`drjit.ArrayBase.state` field.

   .. autoattribute:: Default
      :annotation:

      The default set of optimization flags consisting of

      - :py:attr:`drjit.JitFlag.ConstantPropagation`,
      - :py:attr:`drjit.JitFlag.ValueNumbering`,
      - :py:attr:`drjit.JitFlag.FastMath`,
      - :py:attr:`drjit.JitFlag.SymbolicLoops`,
      - :py:attr:`drjit.JitFlag.OptimizeLoops`,
      - :py:attr:`drjit.JitFlag.SymbolicCalls`,
      - :py:attr:`drjit.JitFlag.MergeFunctions`,
      - :py:attr:`drjit.JitFlag.OptimizeCalls`,
      - :py:attr:`drjit.JitFlag.SymbolicConditionals`,
      - :py:attr:`drjit.JitFlag.ReuseIndices`, and
      - :py:attr:`drjit.JitFlag.ScatterReduceLocal`.

.. autofunction:: set_flag
.. autofunction:: flag
.. autoclass:: scoped_set_flag

   .. automethod:: __init__
   .. automethod:: __enter__
   .. automethod:: __exit__

Type traits
-----------

The functions in this section can be used to infer properties or types of
Dr.Jit arrays.

The naming convention with a trailing ``_v`` or ``_t`` indicates whether a
function returns a value or a type. Evaluation takes place at runtime within
Python. In C++, these expressions are all  ``constexpr`` (i.e., evaluated at
compile time.).

Array type tests
________________

.. autofunction:: is_array_v
.. autofunction:: is_mask_v
.. autofunction:: is_half_v
.. autofunction:: is_float_v
.. autofunction:: is_integral_v
.. autofunction:: is_arithmetic_v
.. autofunction:: is_signed_v
.. autofunction:: is_unsigned_v
.. autofunction:: is_dynamic_v
.. autofunction:: is_jit_v
.. autofunction:: is_diff_v
.. autofunction:: is_vector_v
.. autofunction:: is_complex_v
.. autofunction:: is_matrix_v
.. autofunction:: is_quaternion_v
.. autofunction:: is_tensor_v
.. autofunction:: is_special_v
.. autofunction:: is_struct_v

Array properties (shape, type, etc.)
____________________________________

.. autofunction:: type_v
.. autofunction:: backend_v
.. autofunction:: size_v
.. autofunction:: depth_v
.. autofunction:: itemsize_v

.. py:data:: Dynamic
    :type: int
    :value: -1

    Special size value used to identify dynamic arrays in
    :py:func:`size_v`.

.. py:data:: newaxis
    :type: NoneType
    :value: None

    This variable stores an alias of ``None``. It is used to create new axes in
    tensor slicing operations (analogous to ``np.newaxis`` in NumPy). See the
    discussion of :ref:`tensors <tensors>` for an example.

Access to related types
_______________________

.. autofunction:: mask_t
.. autofunction:: value_t
.. autofunction:: scalar_t
.. autofunction:: int_array_t
.. autofunction:: uint_array_t
.. autofunction:: int32_array_t
.. autofunction:: uint32_array_t
.. autofunction:: int64_array_t
.. autofunction:: uint64_array_t
.. autofunction:: float_array_t
.. autofunction:: float32_array_t
.. autofunction:: float64_array_t
.. autofunction:: replace_type_t
.. autofunction:: detached_t
.. autofunction:: expr_t
.. autofunction:: array_t
.. autofunction:: tensor_t

Bit-level operations
--------------------
.. autofunction:: reinterpret_array
.. autofunction:: popcnt
.. autofunction:: lzcnt
.. autofunction:: tzcnt
.. autofunction:: brev

.. autofunction:: log2i

Standard mathematical functions
-------------------------------

.. autofunction:: fma
.. autofunction:: abs
.. autofunction:: minimum
.. autofunction:: maximum
.. autofunction:: sqrt
.. autofunction:: cbrt
.. autofunction:: rcp
.. autofunction:: rsqrt
.. autofunction:: clip
.. autofunction:: ceil
.. autofunction:: floor
.. autofunction:: trunc
.. autofunction:: round
.. autofunction:: sign
.. autofunction:: copysign
.. autofunction:: mulsign

Operations for vectors and matrices
-----------------------------------

.. autofunction:: cross
.. autofunction:: det
.. autofunction:: diag
.. autofunction:: trace
.. autofunction:: matmul
.. autofunction:: hypot
.. autofunction:: normalize
.. autofunction:: lerp


Operations for complex values and quaternions
---------------------------------------------

.. autofunction:: conj
.. autofunction:: arg
.. autofunction:: real
.. autofunction:: imag

Transcendental functions
------------------------

Dr.Jit implements the most common transcendental functions using methods that
are based on the CEPHES math library. The accuracy of these approximations is
documented in a set of :ref:`tables <transcendental-accuracy>` below.

Trigonometric functions
_______________________

.. autofunction:: sin
.. autofunction:: cos
.. autofunction:: sincos
.. autofunction:: tan
.. autofunction:: asin
.. autofunction:: acos
.. autofunction:: atan
.. autofunction:: atan2

Hyperbolic functions
____________________

.. autofunction:: sinh
.. autofunction:: cosh
.. autofunction:: sincosh
.. autofunction:: tanh
.. autofunction:: asinh
.. autofunction:: acosh
.. autofunction:: atanh

Exponentials, logarithms, power function
________________________________________

.. autofunction:: log2
.. autofunction:: log
.. autofunction:: exp2
.. autofunction:: exp
.. autofunction:: power

Other
_____

.. autofunction:: erf
.. autofunction:: erfinv
.. autofunction:: lgamma
.. autofunction:: rad2deg
.. autofunction:: deg2rad

Safe mathematical functions
---------------------------

Dr.Jit provides "safe" variants of a few standard mathematical operations that
are prone to out-of-domain errors in calculations with floating point rounding
errors.  Such errors could, e.g., cause the argument of a square root to become
negative, which would ordinarily require complex arithmetic. At zero, the
derivative of the square root function is infinite. The following operations
clamp the input to a safe range to avoid these extremes.

.. autofunction:: safe_sqrt
.. autofunction:: safe_asin
.. autofunction:: safe_acos

Automatic differentiation
-------------------------

.. autoclass:: ADMode

   Enumeration to distinguish different types of primal/derivative computation.

   See also :py:func:`drjit.enqueue()`, :py:func:`drjit.traverse()`.

   .. autoattribute:: Primal
      :annotation:

      Primal/original computation without derivative tracking. Note that this
      is *not* a valid input to Dr.Jit AD routines, but it is sometimes useful
      to have this entry when to indicate to a computation that derivative
      propagation should not be performed.

   .. autoattribute:: Forward
      :annotation:

      Propagate derivatives in forward mode (from inputs to outputs)

   .. autoattribute:: Backward
      :annotation:

      Propagate derivatives in backward/reverse mode (from outputs to inputs)

.. autoclass:: ADFlag

   By default, Dr.Jit's AD system destructs the enqueued input graph during
   forward/backward mode traversal. This frees up resources, which is useful
   when working with large wavefronts or very complex computation graphs.
   However, this also prevents repeated propagation of gradients through a
   shared subgraph that is being differentiated multiple times.

   To support more fine-grained use cases that require this, the flags in the
   following enumeration can be used to control what should and should not be
   destructed.

   See also :py:func:`drjit.traverse()`, :py:func:`drjit.forward_from()`,
   :py:func:`drjit.forward_to()`, :py:func:`drjit.backward_from()`, and
   :py:func:`drjit.backward_to()`.

   .. autoattribute:: ClearNone
      :annotation:

      Clear nothing.

   .. autoattribute:: ClearEdges
      :annotation:

      Delete all traversed edges from the computation graph

   .. autoattribute:: ClearInput
      :annotation:

      Clear the gradients of processed input vertices (in-degree == 0)

   .. autoattribute:: ClearInterior
      :annotation:

      Clear the gradients of processed interior vertices (out-degree != 0)

   .. autoattribute:: ClearVertices
      :annotation:

      Clear gradients of processed vertices only, but leave edges intact. Equal
      to ``ClearInput | ClearInterior``.

   .. autoattribute:: AllowNoGrad
      :annotation:

      Don't fail when the input to a ``drjit.forward`` or ``backward``
      operation is not a differentiable array.";

   .. autoattribute:: Default
      :annotation:

      Default: clear everything (edges, gradients of processed vertices). Equal
      to ``ClearEdges | ClearVertices``.

.. autofunction:: detach
.. autofunction:: enable_grad
.. autofunction:: disable_grad
.. autofunction:: set_grad_enabled
.. autofunction:: grad_enabled
.. autofunction:: grad
.. autofunction:: set_grad
.. autofunction:: accum_grad
.. autofunction:: replace_grad
.. autofunction:: clear_grad
.. autofunction:: traverse
.. autofunction:: enqueue
.. autofunction:: forward_from
.. autofunction:: forward_to
.. autofunction:: forward
.. autofunction:: backward_from
.. autofunction:: backward_to
.. autofunction:: backward
.. autofunction:: suspend_grad
.. autofunction:: resume_grad
.. autofunction:: isolate_grad

.. autoclass:: CustomOp

   .. automethod:: eval
   .. automethod:: forward
   .. automethod:: backward
   .. automethod:: name
   .. automethod:: grad_out
   .. automethod:: set_grad_out
   .. automethod:: grad_in
   .. automethod:: set_grad_in
   .. automethod:: add_input
   .. automethod:: add_output

.. autofunction:: custom
.. autofunction:: wrap


Constants
---------

.. data:: e

   The exponential constant :math:`e` represented as a Python ``float``.

.. data:: log_two

   The value :math:`\log(2)` represented as a Python ``float``.

.. data:: inv_log_two

   The value :math:`\frac{1}{\log(2)}` represented as a Python ``float``.

.. data:: pi

   The value :math:`\pi` represented as a Python ``float``.

.. data:: inv_pi

   The value :math:`\frac{1}{\pi}` represented as a Python ``float``.

.. data:: sqrt_pi

   The value :math:`\sqrt{\pi}` represented as a Python ``float``.

.. data:: inv_sqrt_pi

   The value :math:`\frac{1}{\sqrt{\pi}}` represented as a Python ``float``.

.. data:: two_pi

   The value :math:`2\pi` represented as a Python ``float``.

.. data:: inv_two_pi

   The value :math:`\frac{1}{2\pi}` represented as a Python ``float``.

.. data:: sqrt_two_pi

   The value :math:`\sqrt{2\pi}` represented as a Python ``float``.

.. data:: inv_sqrt_two_pi
   :annotation:

   The value :math:`\frac{1}{\sqrt{2\pi}}` represented as a Python ``float``.

.. data:: four_pi

   The value :math:`4\pi` represented as a Python ``float``.

.. data:: inv_four_pi

   The value :math:`\frac{1}{4\pi}` represented as a Python ``float``.

.. data:: sqrt_four_pi

   The value :math:`\sqrt{4\pi}` represented as a Python ``float``.

.. data:: sqrt_two

   The value :math:`\sqrt{2\pi}` represented as a Python ``float``.

.. data:: inv_sqrt_two

   The value :math:`\frac{1}{\sqrt{2\pi}}` represented as a Python ``float``.

.. data:: inf

   The value ``float('inf')`` represented as a Python ``float``.

.. data:: nan

   The value ``float('nan')`` represented as a Python ``float``.

.. autofunction:: epsilon
.. autofunction:: one_minus_epsilon
.. autofunction:: recip_overflow
.. autofunction:: smallest
.. autofunction:: largest

Array base class
----------------

.. autoclass:: ArrayBase

    .. autoproperty:: array
    .. autoproperty:: ndim
    .. autoproperty:: shape
    .. autoproperty:: state
    .. autoproperty:: x
    .. autoproperty:: y
    .. autoproperty:: z
    .. autoproperty:: w
    .. autoproperty:: T
    .. autoproperty:: index
    .. autoproperty:: index_ad
    .. autoproperty:: grad
    .. automethod:: __len__
    .. automethod:: __iter__
    .. automethod:: __repr__
    .. automethod:: __bool__

       Casts the array to a Python ``bool`` type. This is only permissible when
       ``self`` represents an boolean array of both depth and size 1.

    .. automethod:: __add__
    .. automethod:: __radd__
    .. automethod:: __iadd__
    .. automethod:: __sub__
    .. automethod:: __rsub__
    .. automethod:: __isub__
    .. automethod:: __mul__
    .. automethod:: __rmul__
    .. automethod:: __imul__
    .. automethod:: __matmul__
    .. automethod:: __rmatmul__
    .. automethod:: __imatmul__
    .. automethod:: __truediv__
    .. automethod:: __rtruediv__
    .. automethod:: __itruediv__
    .. automethod:: __floordiv__
    .. automethod:: __rfloordiv__
    .. automethod:: __ifloordiv__
    .. automethod:: __mod__
    .. automethod:: __rmod__
    .. automethod:: __imod__
    .. automethod:: __rshift__
    .. automethod:: __rrshift__
    .. automethod:: __irshift__
    .. automethod:: __lshift__
    .. automethod:: __rlshift__
    .. automethod:: __ilshift__
    .. automethod:: __and__
    .. automethod:: __rand__
    .. automethod:: __iand__
    .. automethod:: __or__
    .. automethod:: __ror__
    .. automethod:: __ior__
    .. automethod:: __xor__
    .. automethod:: __rxor__
    .. automethod:: __ixor__
    .. automethod:: __abs__
    .. automethod:: __le__
    .. automethod:: __lt__
    .. automethod:: __ge__
    .. automethod:: __gt__
    .. automethod:: __ne__
    .. automethod:: __eq__
    .. automethod:: __dlpack__
    .. automethod:: __array__
    .. automethod:: numpy
    .. automethod:: torch
    .. automethod:: jax
    .. automethod:: tf


Computation graph analysis
--------------------------

The following operations visualize the contents of Dr.Jit's computation graphs
(of which there are *two*: one for Jit compilation, and one for automatic
differentiation).

.. autofunction:: graphviz
.. autofunction:: graphviz_ad
.. autofunction:: whos
.. autofunction:: whos_ad
.. autofunction:: set_label

Debugging
---------

.. autofunction:: assert_true
.. autofunction:: assert_false
.. autofunction:: assert_equal
.. autofunction:: print
.. autofunction:: format
.. autofunction:: log_level
.. autofunction:: set_log_level

Profiling
---------

.. autofunction:: profile_mark
.. autofunction:: profile_range

Low-level bits
--------------

.. autofunction:: set_backend
.. autofunction:: thread_count
.. autofunction:: set_thread_count
.. autofunction:: sync_thread
.. autofunction:: flush_kernel_cache
.. autofunction:: flush_malloc_cache
.. autofunction:: expand_threshold
.. autofunction:: set_expand_threshold
.. autofunction:: kernel_history
.. autofunction:: kernel_history_clear

Typing
------

.. autoattribute:: T
.. autoattribute:: Ts
.. autoattribute:: AnyArray
